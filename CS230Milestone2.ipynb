{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passenger Screening Algorithm Challenge - Code base\n",
    "\n",
    "Note: we used open source starter code provided as a Kernel on the Kaggle competition page by Brian Farrar at https://www.kaggle.com/jbfarrar/preprocessing-pipeline-and-convnet-trainer, mainly for preprocessing the data.\n",
    "\n",
    "For this milestone, we compared Logistic Regression, AlexNet, VGG16 and VGG19 architectures, training on a small subset of only 6 passengers. \n",
    "\n",
    "The main steps of the program are described below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all the imports at the beginning of the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, conv_1d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the constants used throughout the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------\n",
    "# read_header(infile):  takes an aps file and creates a dict of the data\n",
    "#\n",
    "# infile:               an aps file\n",
    "#\n",
    "# returns:              all of the fields in the header\n",
    "#----------------------------------------------------------------------------------\n",
    "#import tsahelper as tsa\n",
    "#---------------------------------------------------------------------------------------\n",
    "# Constants\n",
    "#\n",
    "# INPUT_FOLDER:                 The folder that contains the source data\n",
    "#\n",
    "# PREPROCESSED_DATA_FOLDER:     The folder that contains preprocessed .npy files \n",
    "# \n",
    "# STAGE1_LABELS:                The CSV file containing the labels by subject\n",
    "#\n",
    "# THREAT_ZONE:                  Threat Zone to train on (actual number not 0 based)\n",
    "#\n",
    "# BATCH_SIZE:                   Number of Subjects per batch\n",
    "#\n",
    "# EXAMPLES_PER_SUBJECT          Number of examples generated per subject\n",
    "#\n",
    "# FILE_LIST:                    A list of the preprocessed .npy files to batch\n",
    "# \n",
    "# TRAIN_TEST_SPLIT_RATIO:       Ratio to split the FILE_LIST between train and test\n",
    "#\n",
    "# TRAIN_SET_FILE_LIST:          The list of .npy files to be used for training\n",
    "#\n",
    "# TEST_SET_FILE_LIST:           The list of .npy files to be used for testing\n",
    "#\n",
    "# IMAGE_DIM:                    The height and width of the images in pixels\n",
    "#\n",
    "# LEARNING_RATE                 Learning rate for the neural network\n",
    "#\n",
    "# N_TRAIN_STEPS                 The number of train steps (epochs) to run\n",
    "#\n",
    "# TRAIN_PATH                    Place to store the tensorboard logs\n",
    "#\n",
    "# MODEL_PATH                    Path where model files are stored\n",
    "#\n",
    "# MODEL_NAME                    Name of the model files\n",
    "#\n",
    "#----------------------------------------------------------------------------------------\n",
    "INPUT_FOLDER = 'tsa_datasets/stage1/aps/stage1_aps'\n",
    "PREPROCESSED_DATA_FOLDER = 'small/preprocessed/'\n",
    "STAGE1_LABELS = 'tsa_datasets/stage1_labels.csv'\n",
    "THREAT_ZONE = 1\n",
    "BATCH_SIZE = 2\n",
    "EXAMPLES_PER_SUBJECT = 182\n",
    "\n",
    "FILE_LIST = []\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.2\n",
    "TRAIN_SET_FILE_LIST = []\n",
    "TEST_SET_FILE_LIST = []\n",
    "\n",
    "IMAGE_DIM = 250\n",
    "LEARNING_RATE = 1e-4\n",
    "N_TRAIN_STEPS = 1\n",
    "TRAIN_PATH_ALEXNET = 'tsa_logs/train/alexnet'\n",
    "TRAIN_PATH_VGG16 = 'tsa_logs/train/vgg16'\n",
    "TRAIN_PATH_VGG19 = 'tsa_logs/train/vgg19'\n",
    "TRAIN_PATH_LOGISTIC = 'tsa_logs/train/logistic'\n",
    "MODEL_PATH_VGG16 = 'tsa_logs/model/vgg16'\n",
    "MODEL_PATH_ALEXNET = 'tsa_logs/model/alexnet'\n",
    "MODEL_PATH_VGG19 = 'tsa_logs/model/vgg19'\n",
    "MODEL_PATH_LOGISTIC = 'tsa_logs/model/logistic'\n",
    "MODEL_NAME_ALEXNET = ('tsa-{}-lr-{}-{}-{}-tz-{}'.format('alexnet-v0.1', LEARNING_RATE, IMAGE_DIM, IMAGE_DIM, THREAT_ZONE ))\n",
    "MODEL_NAME_VGG16 = ('tsa-{}-lr-{}-{}-{}-tz-{}'.format('vgg16-v0.1', LEARNING_RATE, IMAGE_DIM, IMAGE_DIM, THREAT_ZONE )) \n",
    "MODEL_NAME_VGG19 = ('tsa-{}-lr-{}-{}-{}-tz-{}'.format('vgg19-v0.1', LEARNING_RATE, IMAGE_DIM, IMAGE_DIM, THREAT_ZONE ))\n",
    "MODEL_NAME_LOGISTIC = ('tsa-{}-lr-{}-{}-{}-tz-{}'.format('logistic-v0.1', LEARNING_RATE, IMAGE_DIM, IMAGE_DIM, THREAT_ZONE )) \n",
    "# constants\n",
    "# constants\n",
    "# constants\n",
    "COLORMAP = 'pink'\n",
    "BODY_ZONES = 'tsa_datasets/stage1/body_zones.png'\n",
    "THREAT_LABELS = 'tsa_datasets/stage1/stage1_labels.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the 16 sectors, corresponding to the 16 threat zones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the available space on an image into 16 sectors. In the [0] image these\n",
    "# zones correspond to the TSA threat zones.  But on rotated images, the slice\n",
    "# list uses the sector that best shows the threat zone\n",
    "sector01_pts = np.array([[0,160],[200,160],[200,230],[0,230]], np.int32)\n",
    "sector02_pts = np.array([[0,0],[200,0],[200,160],[0,160]], np.int32)\n",
    "sector03_pts = np.array([[330,160],[512,160],[512,240],[330,240]], np.int32)\n",
    "sector04_pts = np.array([[350,0],[512,0],[512,160],[350,160]], np.int32)\n",
    "sector05_pts = np.array([[0,220],[512,220],[512,300],[0,300]], np.int32) # sector 5 is used for both threat zone 5 and 17\n",
    "sector06_pts = np.array([[0,300],[256,300],[256,360],[0,360]], np.int32)\n",
    "sector07_pts = np.array([[256,300],[512,300],[512,360],[256,360]], np.int32)\n",
    "sector08_pts = np.array([[0,370],[225,370],[225,450],[0,450]], np.int32)\n",
    "sector09_pts = np.array([[225,370],[275,370],[275,450],[225,450]], np.int32)\n",
    "sector10_pts = np.array([[275,370],[512,370],[512,450],[275,450]], np.int32)\n",
    "sector11_pts = np.array([[0,450],[256,450],[256,525],[0,525]], np.int32)\n",
    "sector12_pts = np.array([[256,450],[512,450],[512,525],[256,525]], np.int32)\n",
    "sector13_pts = np.array([[0,525],[256,525],[256,600],[0,600]], np.int32)\n",
    "sector14_pts = np.array([[256,525],[512,525],[512,600],[256,600]], np.int32)\n",
    "sector15_pts = np.array([[0,600],[256,600],[256,660],[0,660]], np.int32)\n",
    "sector16_pts = np.array([[256,600],[512,600],[512,660],[256,660]], np.int32)\n",
    "\n",
    "# crop dimensions, upper left x, y, width, height\n",
    "sector_crop_list = [[ 50,  50, 250, 250], # sector 1\n",
    "                    [  0,   0, 250, 250], # sector 2\n",
    "                    [ 50, 250, 250, 250], # sector 3\n",
    "                    [250,   0, 250, 250], # sector 4\n",
    "                    [150, 150, 250, 250], # sector 5/17\n",
    "                    [200, 100, 250, 250], # sector 6\n",
    "                    [200, 150, 250, 250], # sector 7\n",
    "                    [250,  50, 250, 250], # sector 8\n",
    "                    [250, 150, 250, 250], # sector 9\n",
    "                    [300, 200, 250, 250], # sector 10\n",
    "                    [400, 100, 250, 250], # sector 11\n",
    "                    [350, 200, 250, 250], # sector 12\n",
    "                    [410,   0, 250, 250], # sector 13\n",
    "                    [410, 200, 250, 250], # sector 14\n",
    "                    [410,   0, 250, 250], # sector 15\n",
    "                    [410, 200, 250, 250], # sector 16\n",
    "                   ]\n",
    "\n",
    "# Each element in the zone_slice_list contains the sector to use in the call to roi()\n",
    "zone_slice_list = [ [ # threat zone 1\n",
    "                      sector01_pts, sector01_pts, sector01_pts, None, None, None, sector03_pts, sector03_pts, \n",
    "                      sector03_pts, sector03_pts, sector03_pts, None, None, sector01_pts, sector01_pts, sector01_pts ],       \n",
    "                    [ # threat zone 2\n",
    "                      sector02_pts, sector02_pts, sector02_pts, None, None, None, sector04_pts, sector04_pts, \n",
    "                      sector04_pts, sector04_pts, sector04_pts, None, None, sector02_pts, sector02_pts, sector02_pts ],\n",
    "                    [ # threat zone 3\n",
    "                      sector03_pts, sector03_pts, sector03_pts, sector03_pts, None, None, sector01_pts, sector01_pts,\n",
    "                      sector01_pts, sector01_pts, sector01_pts, sector01_pts, None, None, sector03_pts, sector03_pts ],\n",
    "                    [ # threat zone 4\n",
    "                      sector04_pts, sector04_pts, sector04_pts, sector04_pts, None, None, sector02_pts, sector02_pts, \n",
    "                      sector02_pts, sector02_pts, sector02_pts, sector02_pts, None, None, sector04_pts, sector04_pts ],\n",
    "                    [ # threat zone 5\n",
    "                      sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts,\n",
    "                      None, None, None, None, None, None, None, None ],\n",
    "                    [ # threat zone 6\n",
    "                      sector06_pts, None, None, None, None, None, None, None, \n",
    "                      sector07_pts, sector07_pts, sector06_pts, sector06_pts, sector06_pts, sector06_pts, sector06_pts, sector06_pts ],\n",
    "                    [ # threat zone 7\n",
    "                      sector07_pts, sector07_pts, sector07_pts, sector07_pts, sector07_pts, sector07_pts, sector07_pts, sector07_pts, \n",
    "                      None, None, None, None, None, None, None, None ],\n",
    "                    [ # threat zone 8\n",
    "                      sector08_pts, sector08_pts, None, None, None, None, None, sector10_pts, \n",
    "                      sector10_pts, sector10_pts, sector10_pts, sector10_pts, sector08_pts, sector08_pts, sector08_pts, sector08_pts ],\n",
    "                    [ # threat zone 9\n",
    "                      sector09_pts, sector09_pts, sector08_pts, sector08_pts, sector08_pts, None, None, None,\n",
    "                      sector09_pts, sector09_pts, None, None, None, None, sector10_pts, sector09_pts ],\n",
    "                    [ # threat zone 10\n",
    "                      sector10_pts, sector10_pts, sector10_pts, sector10_pts, sector10_pts, sector08_pts, sector10_pts, None, \n",
    "                      None, None, None, None, None, None, None, sector10_pts ],\n",
    "                    [ # threat zone 11\n",
    "                      sector11_pts, sector11_pts, sector11_pts, sector11_pts, None, None, sector12_pts, sector12_pts,\n",
    "                      sector12_pts, sector12_pts, sector12_pts, None, sector11_pts, sector11_pts, sector11_pts, sector11_pts ],\n",
    "                    [ # threat zone 12\n",
    "                      sector12_pts, sector12_pts, sector12_pts, sector12_pts, sector12_pts, sector11_pts, sector11_pts, sector11_pts, \n",
    "                      sector11_pts, sector11_pts, sector11_pts, None, None, sector12_pts, sector12_pts, sector12_pts ],\n",
    "                    [ # threat zone 13\n",
    "                      sector13_pts, sector13_pts, sector13_pts, sector13_pts, None, None, sector14_pts, sector14_pts,\n",
    "                      sector14_pts, sector14_pts, sector14_pts, None, sector13_pts, sector13_pts, sector13_pts, sector13_pts ],\n",
    "                    [ # sector 14\n",
    "                      sector14_pts, sector14_pts, sector14_pts, sector14_pts, sector14_pts, None, sector13_pts, sector13_pts, \n",
    "                      sector13_pts, sector13_pts, sector13_pts, None, None, None, None, None ],\n",
    "                    [ # threat zone 15\n",
    "                      sector15_pts, sector15_pts, sector15_pts, sector15_pts, None, None, sector16_pts, sector16_pts,\n",
    "                      sector16_pts, sector16_pts, None, sector15_pts, sector15_pts, None, sector15_pts, sector15_pts ],\n",
    "                    [ # threat zone 16\n",
    "                      sector16_pts, sector16_pts, sector16_pts, sector16_pts, sector16_pts, sector16_pts, sector15_pts, sector15_pts, \n",
    "                      sector15_pts, sector15_pts, sector15_pts, None, None, None, sector16_pts, sector16_pts ],\n",
    "                    [ # threat zone 17\n",
    "                      None, None, None, None, None, None, None, None,\n",
    "                      sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts, sector05_pts ] ]\n",
    "\n",
    "# Each element in the zone_slice_list contains the sector to use in the call to roi()\n",
    "zone_crop_list =  [ [ # threat zone 1\n",
    "                      sector_crop_list[0], sector_crop_list[0], sector_crop_list[0], None, None, None, \n",
    "                      sector_crop_list[2], sector_crop_list[2], sector_crop_list[2], sector_crop_list[2], sector_crop_list[2], \n",
    "                      None, None, sector_crop_list[0], sector_crop_list[0], sector_crop_list[0] ],       \n",
    "                    [ # threat zone 2\n",
    "                      sector_crop_list[1], sector_crop_list[1], sector_crop_list[1], None, None, None, sector_crop_list[3],\n",
    "                      sector_crop_list[3], sector_crop_list[3], sector_crop_list[3], sector_crop_list[3], None, None,\n",
    "                      sector_crop_list[1], sector_crop_list[1], sector_crop_list[1] ],\n",
    "                    [ # threat zone 3\n",
    "                      sector_crop_list[2], sector_crop_list[2], sector_crop_list[2], sector_crop_list[2], None, None,\n",
    "                      sector_crop_list[0], sector_crop_list[0], sector_crop_list[0], sector_crop_list[0], sector_crop_list[0],\n",
    "                      sector_crop_list[0], None, None, sector_crop_list[2], sector_crop_list[2] ],\n",
    "                    [ # threat zone 4\n",
    "                      sector_crop_list[3], sector_crop_list[3], sector_crop_list[3], sector_crop_list[3], None, None,\n",
    "                      sector_crop_list[1], sector_crop_list[1], sector_crop_list[1], sector_crop_list[1], sector_crop_list[1],\n",
    "                      sector_crop_list[1], None, None, sector_crop_list[3], sector_crop_list[3] ],\n",
    "                    [ # threat zone 5\n",
    "                      sector_crop_list[4], sector_crop_list[4], sector_crop_list[4], sector_crop_list[4], sector_crop_list[4],\n",
    "                      sector_crop_list[4], sector_crop_list[4], sector_crop_list[4],\n",
    "                      None, None, None, None, None, None, None, None ],\n",
    "                    [ # threat zone 6\n",
    "                      sector_crop_list[5], None, None, None, None, None, None, None, \n",
    "                      sector_crop_list[6], sector_crop_list[6], sector_crop_list[5], sector_crop_list[5], sector_crop_list[5],\n",
    "                      sector_crop_list[5], sector_crop_list[5], sector_crop_list[5] ],\n",
    "                    [ # threat zone 7\n",
    "                      sector_crop_list[6], sector_crop_list[6], sector_crop_list[6], sector_crop_list[6], sector_crop_list[6],\n",
    "                      sector_crop_list[6], sector_crop_list[6], sector_crop_list[6], \n",
    "                      None, None, None, None, None, None, None, None ],\n",
    "                    [ # threat zone 8\n",
    "                      sector_crop_list[7], sector_crop_list[7], None, None, None, None, None, sector_crop_list[9], \n",
    "                      sector_crop_list[9], sector_crop_list[9], sector_crop_list[9], sector_crop_list[9], sector_crop_list[7],\n",
    "                      sector_crop_list[7], sector_crop_list[7], sector_crop_list[7] ],\n",
    "                    [ # threat zone 9\n",
    "                      sector_crop_list[8], sector_crop_list[8], sector_crop_list[7], sector_crop_list[7], sector_crop_list[7], None,\n",
    "                      None, None, sector_crop_list[8], sector_crop_list[8], None, None, None, None, sector_crop_list[9],\n",
    "                      sector_crop_list[8] ],\n",
    "                    [ # threat zone 10\n",
    "                      sector_crop_list[9], sector_crop_list[9], sector_crop_list[9], sector_crop_list[9], sector_crop_list[9],\n",
    "                      sector_crop_list[7], sector_crop_list[9], None, \n",
    "                      None, None, None, None, None, None, None, sector_crop_list[9] ],\n",
    "                    [ # threat zone 11\n",
    "                      sector_crop_list[10], sector_crop_list[10], sector_crop_list[10], sector_crop_list[10], None, None,\n",
    "                      sector_crop_list[11], sector_crop_list[11], sector_crop_list[11], sector_crop_list[11], sector_crop_list[11],\n",
    "                      None, sector_crop_list[10], sector_crop_list[10], sector_crop_list[10], sector_crop_list[10] ],\n",
    "                    [ # threat zone 12\n",
    "                      sector_crop_list[11], sector_crop_list[11], sector_crop_list[11], sector_crop_list[11], sector_crop_list[11],\n",
    "                      sector_crop_list[11], sector_crop_list[11], sector_crop_list[11], \n",
    "                      sector_crop_list[11], sector_crop_list[11], sector_crop_list[11], None, None, sector_crop_list[11],\n",
    "                      sector_crop_list[11], sector_crop_list[11] ],\n",
    "                    [ # threat zone 13\n",
    "                      sector_crop_list[12], sector_crop_list[12], sector_crop_list[12], sector_crop_list[12], None, None,\n",
    "                      sector_crop_list[13], sector_crop_list[13], sector_crop_list[13], sector_crop_list[13], sector_crop_list[13],\n",
    "                      None, sector_crop_list[12], sector_crop_list[12], sector_crop_list[12], sector_crop_list[12] ],\n",
    "                    [ # sector 14\n",
    "                      sector_crop_list[13], sector_crop_list[13], sector_crop_list[13], sector_crop_list[13], sector_crop_list[13],\n",
    "                      None, sector_crop_list[13], sector_crop_list[13], \n",
    "                      sector_crop_list[12], sector_crop_list[12], sector_crop_list[12], None, None, None, None, None ],\n",
    "                    [ # threat zone 15\n",
    "                      sector_crop_list[14], sector_crop_list[14], sector_crop_list[14], sector_crop_list[14], None, None,\n",
    "                      sector_crop_list[15], sector_crop_list[15],\n",
    "                      sector_crop_list[15], sector_crop_list[15], None, sector_crop_list[14], sector_crop_list[14], None,\n",
    "                      sector_crop_list[14], sector_crop_list[14] ],\n",
    "                    [ # threat zone 16\n",
    "                      sector_crop_list[15], sector_crop_list[15], sector_crop_list[15], sector_crop_list[15], sector_crop_list[15],\n",
    "                      sector_crop_list[15], sector_crop_list[14], sector_crop_list[14], \n",
    "                      sector_crop_list[14], sector_crop_list[14], sector_crop_list[14], None, None, None, sector_crop_list[15],\n",
    "                      sector_crop_list[15] ],\n",
    "                    [ # threat zone 17\n",
    "                      None, None, None, None, None, None, None, None,\n",
    "                      sector_crop_list[4], sector_crop_list[4], sector_crop_list[4], sector_crop_list[4], sector_crop_list[4],\n",
    "                      sector_crop_list[4], sector_crop_list[4], sector_crop_list[4] ] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below reads in the header of a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_header(infile):\n",
    "    # declare dictionary\n",
    "    h = dict()\n",
    "    \n",
    "    with open(infile, 'r+b') as fid:\n",
    "\n",
    "        h['filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "        h['parent_filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n",
    "        h['comments1'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "        h['comments2'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n",
    "        h['energy_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['config_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['file_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['trans_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['scan_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['data_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['date_modified'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 16))\n",
    "        h['frequency'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['mat_velocity'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['num_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "        h['num_polarization_channels'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['spare00'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['adc_min_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['adc_max_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['band_width'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['spare01'] = np.fromfile(fid, dtype = np.int16, count = 5)\n",
    "        h['polarization_type'] = np.fromfile(fid, dtype = np.int16, count = 4)\n",
    "        h['record_header_size'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['word_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['word_precision'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['min_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['max_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['avg_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['data_scale_factor'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['data_units'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['surf_removal'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "        h['edge_weighting'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "        h['x_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "        h['y_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "        h['z_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "        h['t_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n",
    "        h['spare02'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['x_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['scan_orientation'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['scan_direction'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['data_storage_order'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['scanner_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['x_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['t_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['num_x_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "        h['num_y_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "        h['num_z_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "        h['num_t_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n",
    "        h['x_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['x_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['x_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['x_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['date_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "        h['time_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n",
    "        h['depth_recon'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['x_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['elevation_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['roll_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['azimuth_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['adc_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['spare06'] = np.fromfile(fid, dtype = np.int16, count = 1)\n",
    "        h['scanner_radius'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['x_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['y_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['z_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['t_delay'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['range_gate_start'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['range_gate_end'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['ahis_software_version'] = np.fromfile(fid, dtype = np.float32, count = 1)\n",
    "        h['spare_end'] = np.fromfile(fid, dtype = np.float32, count = 10)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads data from one image file, which can be of any of the 4 extensions. For this milestone, we worked with .aps extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "# read_data(infile):  reads and rescales any of the four image types\n",
    "#\n",
    "# infile:             an .aps, .aps3d, .a3d, or ahi file\n",
    "#\n",
    "# returns:            the stack of images\n",
    "#\n",
    "# note:               word_type == 7 is an np.float32, word_type == 4 is np.uint16      \n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "def read_data(infile):\n",
    "    \n",
    "    # read in header and get dimensions\n",
    "    h = read_header(infile)\n",
    "    nx = int(h['num_x_pts'])\n",
    "    ny = int(h['num_y_pts'])\n",
    "    nt = int(h['num_t_pts'])\n",
    "    \n",
    "    extension = os.path.splitext(infile)[1]\n",
    "    \n",
    "    with open(infile, 'rb') as fid:\n",
    "          \n",
    "        # skip the header\n",
    "        fid.seek(512) \n",
    "\n",
    "        # handle .aps and .a3aps files\n",
    "        if extension == '.aps' or extension == '.a3daps':\n",
    "        \n",
    "            if(h['word_type']==7):\n",
    "                data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n",
    "\n",
    "            elif(h['word_type']==4): \n",
    "                data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n",
    "\n",
    "            # scale and reshape the data\n",
    "            data = data * h['data_scale_factor'] \n",
    "            data = data.reshape(nx, ny, nt, order='F').copy()\n",
    "\n",
    "        # handle .a3d files\n",
    "        elif extension == '.a3d':\n",
    "              \n",
    "            if(h['word_type']==7):\n",
    "                data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n",
    "                \n",
    "            elif(h['word_type']==4):\n",
    "                data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n",
    "\n",
    "            # scale and reshape the data\n",
    "            data = data * h['data_scale_factor']\n",
    "            data = data.reshape(nx, nt, ny, order='F').copy() \n",
    "            \n",
    "        # handle .ahi files\n",
    "        elif extension == '.ahi':\n",
    "            data = np.fromfile(fid, dtype = np.float32, count = 2* nx * ny * nt)\n",
    "            data = data.reshape(2, ny, nx, nt, order='F').copy()\n",
    "            real = data[0,:,:,:].copy()\n",
    "            imag = data[1,:,:,:].copy()\n",
    "\n",
    "        if extension != '.ahi':\n",
    "            return data\n",
    "        else:\n",
    "            return real, imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the labels for a subject, as threat probabilities for each zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# get_subject_labels(infile, subject_id):  lists threat probabilities by zone for a given subject\n",
    "#\n",
    "# infile:                                      labels csv file\n",
    "#\n",
    "# subject_id:                                  the individual you want the threat zone labels for\n",
    "#\n",
    "# returns:                                     a df with the list of zones and contraband (0 or 1)\n",
    "#\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_subject_labels(infile, subject_id):\n",
    "\n",
    "    # read labels into a dataframe\n",
    "    df = pd.read_csv(infile)\n",
    "\n",
    "    # Separate the zone and subject id into a df\n",
    "    df['Subject'], df['Zone'] = df['Id'].str.split('_',1).str\n",
    "    df = df[['Subject', 'Zone', 'Probability']]\n",
    "    threat_list = df.loc[df['Subject'] == subject_id]\n",
    "    \n",
    "    return threat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the label corresonding to a given zone of a subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# get_subject_zone_label(zone_num, df):        gets a label for a given subject and zone\n",
    "#\n",
    "# zone_num:                                    a 0 based threat zone index\n",
    "#\n",
    "# df:                                          a df like that returned from get_subject_labels(...)\n",
    "#\n",
    "# returns:                                     [0,1] if contraband is present, [1,0] if it isnt\n",
    "#\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_subject_zone_label(zone_num, df):\n",
    "    \n",
    "    # Dict to convert a 0 based threat zone index to the text we need to look up the label\n",
    "    zone_index = {0: 'Zone1', 1: 'Zone2', 2: 'Zone3', 3: 'Zone4', 4: 'Zone5', 5: 'Zone6', 6: 'Zone7', 7: 'Zone8',\n",
    "                  8: 'Zone9', 9: 'Zone10', 10: 'Zone11', 11: 'Zone12', 12: 'Zone13', 13: 'Zone14', 14: 'Zone15', 15: 'Zone16',\n",
    "                  16: 'Zone17'\n",
    "                 }\n",
    "    # get the text key from the dictionary\n",
    "    key = zone_index.get(zone_num)\n",
    "    \n",
    "    # select the probability value and make the label\n",
    "    if df.loc[df['Zone'] == key]['Probability'].values[0] == 1:\n",
    "        # threat present\n",
    "        return [0,1]\n",
    "    else:\n",
    "        #no threat present\n",
    "        return [1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts an image to grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------\n",
    "# convert_to_grayscale(img):           converts a ATI scan to grayscale\n",
    "#\n",
    "# infile:                              an aps file\n",
    "#\n",
    "# returns:                             an image\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "def convert_to_grayscale(img):\n",
    "    # scale pixel values to grayscale\n",
    "    base_range = np.amax(img) - np.amin(img)\n",
    "    rescaled_range = 255 - 0\n",
    "    img_rescaled = (((img - np.amin(img)) * rescaled_range) / base_range)\n",
    "\n",
    "    return np.uint8(img_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies a histogram equalization transformation to a given image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------\n",
    "# spread_spectrum(img):                applies a histogram equalization transformation\n",
    "#\n",
    "# img:                                 a single scan\n",
    "#\n",
    "# returns:                             a transformed scan\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "def spread_spectrum(img):\n",
    "    img = stats.threshold(img, threshmin=12, newval=0)\n",
    "    \n",
    "    # see http://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img= clahe.apply(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a region of interest from the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "# roi(img, vertices):                  uses vertices to mask the image\n",
    "#\n",
    "# img:                                 the image to be masked\n",
    "#\n",
    "# vertices:                            a set of vertices that define the region of interest\n",
    "#\n",
    "# returns:                             a masked image\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "def roi(img, vertices):\n",
    "  \n",
    "    # blank mask\n",
    "    mask = np.zeros_like(img)\n",
    "\n",
    "    # fill the mask\n",
    "    cv2.fillPoly(mask, [vertices], 255)\n",
    "\n",
    "    # now only show the area that is the mask\n",
    "    masked = cv2.bitwise_and(img, mask)\n",
    "    \n",
    "    return masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crops an image to fit a given [x, y, width, height] crop_list entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------\n",
    "# crop(img, crop_list):                uses vertices to mask the image\n",
    "#\n",
    "# img:                                 the image to be cropped\n",
    "#\n",
    "# crop_list:                           a crop_list entry with [x , y, width, height]\n",
    "#\n",
    "# returns:                             a cropped image\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "def crop(img, crop_list):\n",
    "\n",
    "    x_coord = crop_list[0]\n",
    "    y_coord = crop_list[1]\n",
    "    width = crop_list[2]\n",
    "    height = crop_list[3]\n",
    "    \n",
    "    cropped_img = img[x_coord:x_coord+width, y_coord:y_coord+height]\n",
    "    \n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizes an image by making each of the pixel values between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# normalize(image): Take segmented tsa image and normalize pixel values to be between 0 and 1\n",
    "#\n",
    "# parameters:      image - a tsa scan\n",
    "#\n",
    "# returns:         a normalized image\n",
    "#\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def normalize(image):\n",
    "    MIN_BOUND = 0.0\n",
    "    MAX_BOUND = 255.0\n",
    "    \n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image>1] = 1.\n",
    "    image[image<0] = 0.\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centers an image at PIXEL_MEAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# zero_center(image): Shift normalized image data and move the range so it is 0 centered at the PIXEL_MEAN\n",
    "#\n",
    "# parameters:      image\n",
    "#\n",
    "# returns:         a zero centered image\n",
    "#\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def zero_center(image):\n",
    "  \n",
    "    PIXEL_MEAN = 0.014327\n",
    "    \n",
    "    image = image - PIXEL_MEAN\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesses the data. For this milestone, we used only a list of 6 subjects, given in the list SUBJECT_LIST below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# preprocess_tsa_data(): preprocesses the tsa datasets\n",
    "#\n",
    "# parameters:      none\n",
    "#\n",
    "# returns:         none\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_tsa_data():\n",
    "    \n",
    "    # OPTION 1: get a list of all subjects for which there are labels\n",
    "    # df = pd.read_csv(STAGE1_LABELS)\n",
    "    # df['Subject'], df['Zone'] = df['Id'].str.split('_',1).str\n",
    "    # SUBJECT_LIST = df['Subject'].unique()\n",
    "\n",
    "    #OPTION 2: get a list of all subjects for whom there is data\n",
    "    #SUBJECT_LIST = [os.path.splitext(subject)[0] for subject in os.listdir(INPUT_FOLDER)]\n",
    "    \n",
    "    # OPTION 3: get a list of subjects for small bore test purposes\n",
    "    SUBJECT_LIST = ['00360f79fd6e02781457eda48f85da90','0043db5e8c819bffc15261b1f1ac5e42',\n",
    "                    '0050492f92e22eed3474ae3a6fc907fa','006ec59fa59dd80a64c85347eef810c7',\n",
    "                    '0097503ee9fa0606559c56458b281a08','011516ab0eca7cad7f5257672ddde70e']\n",
    "    \n",
    "    # intialize tracking and saving items\n",
    "    batch_num = 1\n",
    "    threat_zone_examples = []\n",
    "    start_time = timer()\n",
    "    \n",
    "    for subject in SUBJECT_LIST:\n",
    "        # read in the images\n",
    "        print('--------------------------------------------------------------')\n",
    "        print('t+> {:5.3f} |Reading images for subject #: {}'.format(timer()-start_time, \n",
    "                                                                     subject))\n",
    "        print('--------------------------------------------------------------')\n",
    "        images = read_data(INPUT_FOLDER + '/' + subject + '.aps')\n",
    "\n",
    "        # transpose so that the slice is the first dimension shape(16, 620, 512)\n",
    "        images = images.transpose()\n",
    "\n",
    "        # for each threat zone, loop through each image, mask off the zone and then crop it\n",
    "        for tz_num, threat_zone_x_crop_dims in enumerate(zip(zone_slice_list, \n",
    "                                                             zone_crop_list)):\n",
    "\n",
    "            threat_zone = threat_zone_x_crop_dims[0]\n",
    "            crop_dims = threat_zone_x_crop_dims[1]\n",
    "\n",
    "            # get label\n",
    "            label = np.array(get_subject_zone_label(tz_num, \n",
    "                             get_subject_labels(STAGE1_LABELS, subject)))\n",
    "\n",
    "            for img_num, img in enumerate(images):\n",
    "\n",
    "                print('Threat Zone:Image -> {}:{}'.format(tz_num, img_num))\n",
    "                print('Threat Zone Label -> {}'.format(label))\n",
    "                \n",
    "                if threat_zone[img_num] is not None:\n",
    "\n",
    "                    # correct the orientation of the image\n",
    "                    print('-> reorienting base image') \n",
    "                    base_img = np.flipud(img)\n",
    "                    print('-> shape {}|mean={}'.format(base_img.shape, \n",
    "                                                       base_img.mean()))\n",
    "\n",
    "                    # convert to grayscale\n",
    "                    print('-> converting to grayscale')\n",
    "                    rescaled_img = convert_to_grayscale(base_img)\n",
    "                    print('-> shape {}|mean={}'.format(rescaled_img.shape, \n",
    "                                                       rescaled_img.mean()))\n",
    "\n",
    "                    # spread the spectrum to improve contrast\n",
    "                    print('-> spreading spectrum')\n",
    "                    high_contrast_img = spread_spectrum(rescaled_img)\n",
    "                    print('-> shape {}|mean={}'.format(high_contrast_img.shape,\n",
    "                                                       high_contrast_img.mean()))\n",
    "\n",
    "                    # get the masked image\n",
    "                    print('-> masking image')\n",
    "                    masked_img = roi(high_contrast_img, threat_zone[img_num])\n",
    "                    print('-> shape {}|mean={}'.format(masked_img.shape, \n",
    "                                                       masked_img.mean()))\n",
    "\n",
    "                    # crop the image\n",
    "                    print('-> cropping image')\n",
    "                    cropped_img = crop(masked_img, crop_dims[img_num])\n",
    "                    print('-> shape {}|mean={}'.format(cropped_img.shape, \n",
    "                                                       cropped_img.mean()))\n",
    "\n",
    "                    # normalize the image\n",
    "                    print('-> normalizing image')\n",
    "                    normalized_img = normalize(cropped_img)\n",
    "                    print('-> shape {}|mean={}'.format(normalized_img.shape, \n",
    "                                                       normalized_img.mean()))\n",
    "\n",
    "                    # zero center the image\n",
    "                    print('-> zero centering')\n",
    "                    zero_centered_img = zero_center(normalized_img)\n",
    "                    print('-> shape {}|mean={}'.format(zero_centered_img.shape, \n",
    "                                                       zero_centered_img.mean()))\n",
    "\n",
    "                    # append the features and labels to this threat zone's example array\n",
    "                    print ('-> appending example to threat zone {}'.format(tz_num))\n",
    "                    threat_zone_examples.append([[tz_num], zero_centered_img, label])\n",
    "                    print ('-> shape {:d}:{:d}:{:d}:{:d}:{:d}:{:d}'.format(\n",
    "                                                         len(threat_zone_examples),\n",
    "                                                         len(threat_zone_examples[0]),\n",
    "                                                         len(threat_zone_examples[0][0]),\n",
    "                                                         len(threat_zone_examples[0][1][0]),\n",
    "                                                         len(threat_zone_examples[0][1][1]),\n",
    "                                                         len(threat_zone_examples[0][2])))\n",
    "                else:\n",
    "                    print('-> No view of tz:{} in img:{}. Skipping to next...'.format( \n",
    "                                tz_num, img_num))\n",
    "                print('------------------------------------------------')\n",
    "\n",
    "        # each subject gets EXAMPLES_PER_SUBJECT number of examples (182 to be exact, \n",
    "        # so this section just writes out the the data once there is a full minibatch \n",
    "        # complete.\n",
    "        if ((len(threat_zone_examples) % (BATCH_SIZE * EXAMPLES_PER_SUBJECT)) == 0):\n",
    "            for tz_num, tz in enumerate(zone_slice_list):\n",
    "                tz_examples_to_save = []\n",
    "\n",
    "                # write out the batch and reset\n",
    "                print(' -> writing: ' + PREPROCESSED_DATA_FOLDER + \n",
    "                                        'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format( \n",
    "                                        tz_num+1,\n",
    "                                        len(threat_zone_examples[0][1][0]),\n",
    "                                        len(threat_zone_examples[0][1][1]), \n",
    "                                        batch_num))\n",
    "\n",
    "                # get this tz's examples\n",
    "                tz_examples = [example for example in threat_zone_examples if example[0] == \n",
    "                               [tz_num]]\n",
    "\n",
    "                # drop unused columns\n",
    "                tz_examples_to_save.append([[features_label[1], features_label[2]] \n",
    "                                            for features_label in tz_examples])\n",
    "\n",
    "                # save batch.  Note that the trainer looks for tz{} where {} is a \n",
    "                # tz_num 1 based in the minibatch file to select which batches to \n",
    "                # use for training a given threat zone\n",
    "                np.save(PREPROCESSED_DATA_FOLDER + \n",
    "                        'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format(tz_num+1, \n",
    "                                                         len(threat_zone_examples[0][1][0]),\n",
    "                                                         len(threat_zone_examples[0][1][1]), \n",
    "                                                         batch_num), \n",
    "                                                         tz_examples_to_save)\n",
    "                del tz_examples_to_save\n",
    "\n",
    "            #reset for next batch \n",
    "            del threat_zone_examples\n",
    "            threat_zone_examples = []\n",
    "            batch_num += 1\n",
    "    \n",
    "    # we may run out of subjects before we finish a batch, so we write out \n",
    "    # the last batch stub\n",
    "    if (len(threat_zone_examples) > 0):\n",
    "        for tz_num, tz in enumerate(zone_slice_list):\n",
    "\n",
    "            tz_examples_to_save = []\n",
    "\n",
    "            # write out the batch and reset\n",
    "            print(' -> writing: ' + PREPROCESSED_DATA_FOLDER \n",
    "                    + 'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format(tz_num+1, \n",
    "                      len(threat_zone_examples[0][1][0]),\n",
    "                      len(threat_zone_examples[0][1][1]), batch_num))\n",
    "\n",
    "            # get this tz's examples\n",
    "            tz_examples = [example for example in threat_zone_examples if example[0] == \n",
    "                           [tz_num]]\n",
    "\n",
    "            # drop unused columns\n",
    "            tz_examples_to_save.append([[features_label[1], features_label[2]] \n",
    "                                        for features_label in tz_examples])\n",
    "\n",
    "            #save batch\n",
    "            np.save(PREPROCESSED_DATA_FOLDER + \n",
    "                    'preprocessed_TSA_scans-tz{}-{}-{}-b{}.npy'.format(tz_num+1, \n",
    "                                                     len(threat_zone_examples[0][1][0]),\n",
    "                                                     len(threat_zone_examples[0][1][1]), \n",
    "                                                     batch_num), \n",
    "                                                     tz_examples_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the files used for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# get_train_test_file_list(): gets the batch file list, splits between train and test\n",
    "#\n",
    "# parameters:      none\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def get_train_test_file_list():\n",
    "    \n",
    "    global FILE_LIST\n",
    "    global TRAIN_SET_FILE_LIST\n",
    "    global TEST_SET_FILE_LIST\n",
    "\n",
    "    if os.listdir(PREPROCESSED_DATA_FOLDER) == []:\n",
    "        print ('No preprocessed data available.  Skipping preprocessed data setup..')\n",
    "    else:\n",
    "        FILE_LIST = [f for f in os.listdir(PREPROCESSED_DATA_FOLDER) \n",
    "                     if re.search(re.compile('-tz' + str(THREAT_ZONE) + '-'), f)]\n",
    "        train_test_split = len(FILE_LIST) - \\\n",
    "                           max(int(len(FILE_LIST)*TRAIN_TEST_SPLIT_RATIO),1)\n",
    "        TRAIN_SET_FILE_LIST = FILE_LIST[:train_test_split]\n",
    "        TEST_SET_FILE_LIST = FILE_LIST[train_test_split:]\n",
    "        print('Train/Test Split -> {} file(s) of {} used for testing'.format( \n",
    "              len(FILE_LIST) - train_test_split, len(FILE_LIST)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares a batch of features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# input_pipeline(filename, path): prepares a batch of features and labels for training\n",
    "#\n",
    "# parameters:      filename - the file to be batched into the model\n",
    "#                  path - the folder where filename resides\n",
    "#\n",
    "# returns:         feature_batch - a batch of features to train or test on\n",
    "#                  label_batch - a batch of labels related to the feature_batch\n",
    "#\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def input_pipeline(filename, path):\n",
    "\n",
    "    preprocessed_tz_scans = []\n",
    "    feature_batch = []\n",
    "    label_batch = []\n",
    "    \n",
    "    #Load a batch of preprocessed tz scans\n",
    "    preprocessed_tz_scans = np.load(os.path.join(path, filename))\n",
    "        \n",
    "    #Shuffle to randomize for input into the model\n",
    "    np.random.shuffle(preprocessed_tz_scans)\n",
    "    \n",
    "    # separate features and labels\n",
    "    for example_list in preprocessed_tz_scans:\n",
    "        for example in example_list:\n",
    "            feature_batch.append(example[0])\n",
    "            label_batch.append(example[1])\n",
    "    \n",
    "    feature_batch = np.asarray(feature_batch, dtype=np.float32)\n",
    "    label_batch = np.asarray(label_batch, dtype=np.float32)\n",
    "    \n",
    "    return feature_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffles the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# shuffle_train_set(): shuffle the list of batch files so that each train step\n",
    "#                      receives them in a different order since the TRAIN_SET_FILE_LIST\n",
    "#                      is a global\n",
    "#\n",
    "# parameters:      train_set - the file listing to be shuffled\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def shuffle_train_set(train_set):\n",
    "    sorted_file_list = random.shuffle(train_set)\n",
    "    TRAIN_SET_FILE_LIST = sorted_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "The implementation of this neural networks was given in the starter code we used from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# alexnet(width, height, lr): defines the alexnet\n",
    "#\n",
    "# parameters:      width - width of the input image\n",
    "#                  height - height of the input image\n",
    "#                  lr - learning rate\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def alexnet(width, height, lr):\n",
    "    network = input_data(shape=[None, width, height, 1], name='features')\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 256, 5, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 384, 3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='tanh')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum', loss='categorical_crossentropy', \n",
    "                         learning_rate=lr, name='labels')\n",
    "\n",
    "    model = tflearn.DNN(network, checkpoint_path=MODEL_PATH_ALEXNET + MODEL_NAME_ALEXNET, \n",
    "                        tensorboard_dir=TRAIN_PATH_ALEXNET, tensorboard_verbose=3, max_checkpoints=1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16\n",
    "This is one of the main contributions we brought to the open-source code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16(width, height, lr):\n",
    "    network = input_data(shape=[None, width, height, 1], name='features')\n",
    "    network = conv_2d(network, 64, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = conv_2d(network, 128, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = conv_2d(network, 256, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    \n",
    "    network = regression(network, optimizer='momentum', loss='categorical_crossentropy', \n",
    "                         learning_rate=lr, name='labels')\n",
    "\n",
    "    model = tflearn.DNN(network, checkpoint_path=MODEL_PATH_VGG16 + MODEL_NAME_VGG16, \n",
    "                        tensorboard_dir=TRAIN_PATH_VGG16, tensorboard_verbose=3, max_checkpoints=1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG 19\n",
    "Similarly to the VGG16, this is one of the main contributions we brought to the open-source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg19(width, height, lr):\n",
    "    network = input_data(shape=[None, width, height, 1], name='features')\n",
    "    network = conv_2d(network, 64, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 3, strides=2)\n",
    "    network = conv_2d(network, 128, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 128, 3, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = conv_2d(network, 256, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 256, 3, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = conv_2d(network, 3, 512, strides=3, activation='relu')\n",
    "    network = max_pool_2d(network, 2, strides=2)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 4096, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='momentum', loss='categorical_crossentropy', \n",
    "                         learning_rate=lr, name='labels')\n",
    "\n",
    "    model = tflearn.DNN(network, checkpoint_path=MODEL_PATH_VGG19 + MODEL_NAME_VGG19, \n",
    "                        tensorboard_dir=TRAIN_PATH_VGG19, tensorboard_verbose=3, max_checkpoints=1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "We also tried a basic logistic regression model, tuning it with different values for the learning rate and trying different activations and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_net(width, height, lr, activation, optimizer):\n",
    "    network = input_data(shape=[None, width, height, 1], name='features')\n",
    "    network = fully_connected(network, 2, activation=activation)\n",
    "\n",
    "    network = regression(network, optimizer=optimizer, loss='categorical_crossentropy', \n",
    "                         learning_rate=lr, name='labels')\n",
    "\n",
    "    model = tflearn.DNN(network, checkpoint_path=MODEL_PATH_LOGISTIC + MODEL_NAME_LOGISTIC, \n",
    "                        tensorboard_dir=TRAIN_PATH_LOGISTIC, tensorboard_verbose=3, max_checkpoints=1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns two arrays, containing the features and labels used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(): \n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "    \n",
    "    # get train and test batches\n",
    "    get_train_test_file_list()\n",
    "    \n",
    "    # read in the validation test set\n",
    "    for j, test_f_in in enumerate(TEST_SET_FILE_LIST):\n",
    "        if j == 0:\n",
    "            val_features, val_labels = input_pipeline(test_f_in, PREPROCESSED_DATA_FOLDER)\n",
    "        else:\n",
    "            tmp_feature_batch, tmp_label_batch = input_pipeline(test_f_in, \n",
    "                                                                PREPROCESSED_DATA_FOLDER)\n",
    "            val_features = np.concatenate((tmp_feature_batch, val_features), axis=0)\n",
    "            val_labels = np.concatenate((tmp_label_batch, val_labels), axis=0)\n",
    "\n",
    "    val_features = val_features.reshape(-1, IMAGE_DIM, IMAGE_DIM, 1)\n",
    "    return [val_features, val_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "The main body of code that trains a neural network with a given model, for a given number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# train_conv_net(): runs the train op\n",
    "#\n",
    "# parameters:      none\n",
    "#\n",
    "# returns:         none\n",
    "#\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "def train_conv_net(model, model_name, num_epoch):\n",
    "    # start training process\n",
    "    for i in range(N_TRAIN_STEPS):\n",
    "        # shuffle the train set files before each step\n",
    "        shuffle_train_set(TRAIN_SET_FILE_LIST)\n",
    "        print(TRAIN_SET_FILE_LIST)\n",
    "        # run through every batch in the training set\n",
    "        for f_in in TRAIN_SET_FILE_LIST:\n",
    "            \n",
    "            # read in a batch of features and labels for training\n",
    "            feature_batch, label_batch = input_pipeline(f_in, PREPROCESSED_DATA_FOLDER)\n",
    "            feature_batch = feature_batch.reshape(-1, IMAGE_DIM, IMAGE_DIM, 1)\n",
    "            print ('Feature Batch Shape ->', feature_batch.shape)                \n",
    "                \n",
    "            # run the fit operation\n",
    "            print(\"training model: \")\n",
    "            print(model.fit({'features': feature_batch}, {'labels': label_batch}, n_epoch=10, \n",
    "                      validation_set=({'features': val_features}, {'labels': val_labels}), \n",
    "                      shuffle=True, snapshot_step=None, show_metric=True, \n",
    "                      run_id=model_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the preprocessing of data is computationally and time intensive, we only preprocess the data once, before starting any training. After we preprocessed the data once, we comment out this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess_tsa_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we generate the train/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test Split -> 1 file(s) of 3 used for testing\n"
     ]
    }
   ],
   "source": [
    "val_features, val_labels = get_features_and_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## AlexNet \n",
    "learning_rate = 10^-4, num_epoch = 10\n",
    "We can see that the loss on the validation set is decreasing, while the accuracy is increasing, approaching 1 pretty fast. This is due to the fact that we only used 6 subjects in our training, so on the entire dataset, we expect to see a more realistic value for accuracy, that does not hit 1. The same holds for all the other models we tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b2.npy', 'preprocessed_TSA_scans-tz1-250-250-b1.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-alexnet-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/alexnet/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 12.402s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.76989 - val_acc: 0.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.70655\u001b[0m\u001b[0m | time: 12.053s\n",
      "| Momentum | epoch: 002 | loss: 0.70655 - acc: 0.2455 | val_loss: 0.76095 - val_acc: 0.0455 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.76898\u001b[0m\u001b[0m | time: 11.731s\n",
      "| Momentum | epoch: 003 | loss: 0.76898 - acc: 0.4165 | val_loss: 0.74832 - val_acc: 0.0909 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.75431\u001b[0m\u001b[0m | time: 12.931s\n",
      "| Momentum | epoch: 004 | loss: 0.75431 - acc: 0.5132 | val_loss: 0.73252 - val_acc: 0.1364 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.73266\u001b[0m\u001b[0m | time: 13.269s\n",
      "| Momentum | epoch: 005 | loss: 0.73266 - acc: 0.5041 | val_loss: 0.71401 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.70461\u001b[0m\u001b[0m | time: 12.902s\n",
      "| Momentum | epoch: 006 | loss: 0.70461 - acc: 0.5891 | val_loss: 0.69327 - val_acc: 0.5455 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.73060\u001b[0m\u001b[0m | time: 13.211s\n",
      "| Momentum | epoch: 007 | loss: 0.73060 - acc: 0.4266 | val_loss: 0.67075 - val_acc: 0.7727 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.67435\u001b[0m\u001b[0m | time: 13.348s\n",
      "| Momentum | epoch: 008 | loss: 0.67435 - acc: 0.5957 | val_loss: 0.64685 - val_acc: 0.9545 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.67474\u001b[0m\u001b[0m | time: 11.738s\n",
      "| Momentum | epoch: 009 | loss: 0.67474 - acc: 0.5691 | val_loss: 0.62186 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.66819\u001b[0m\u001b[0m | time: 12.741s\n",
      "| Momentum | epoch: 010 | loss: 0.66819 - acc: 0.6255 | val_loss: 0.59611 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-alexnet-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/alexnet/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.65244\u001b[0m\u001b[0m | time: 12.801s\n",
      "| Momentum | epoch: 011 | loss: 0.65244 - acc: 0.6522 | val_loss: 0.57508 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.66424\u001b[0m\u001b[0m | time: 12.156s\n",
      "| Momentum | epoch: 012 | loss: 0.66424 - acc: 0.6450 | val_loss: 0.55735 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.65999\u001b[0m\u001b[0m | time: 11.926s\n",
      "| Momentum | epoch: 013 | loss: 0.65999 - acc: 0.6218 | val_loss: 0.54265 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.70204\u001b[0m\u001b[0m | time: 13.765s\n",
      "| Momentum | epoch: 014 | loss: 0.70204 - acc: 0.5348 | val_loss: 0.53084 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.71500\u001b[0m\u001b[0m | time: 11.829s\n",
      "| Momentum | epoch: 015 | loss: 0.71500 - acc: 0.4678 | val_loss: 0.52121 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.72928\u001b[0m\u001b[0m | time: 12.263s\n",
      "| Momentum | epoch: 016 | loss: 0.72928 - acc: 0.4458 | val_loss: 0.51377 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.70462\u001b[0m\u001b[0m | time: 12.261s\n",
      "| Momentum | epoch: 017 | loss: 0.70462 - acc: 0.5144 | val_loss: 0.50889 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.71388\u001b[0m\u001b[0m | time: 14.376s\n",
      "| Momentum | epoch: 018 | loss: 0.71388 - acc: 0.5094 | val_loss: 0.50537 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.70691\u001b[0m\u001b[0m | time: 13.701s\n",
      "| Momentum | epoch: 019 | loss: 0.70691 - acc: 0.5214 | val_loss: 0.50383 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.71567\u001b[0m\u001b[0m | time: 11.785s\n",
      "| Momentum | epoch: 020 | loss: 0.71567 - acc: 0.5145 | val_loss: 0.50355 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_alexnet = tf.Graph()\n",
    "with g_alexnet.as_default():\n",
    "    alexnet_model = alexnet(IMAGE_DIM, IMAGE_DIM, 1e-4)\n",
    "    train_conv_net(alexnet_model, MODEL_NAME_ALEXNET, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 \n",
    "learning_rate = 10^-4, num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b2.npy', 'preprocessed_TSA_scans-tz1-250-250-b1.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg16-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/vgg16/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 34.258s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.69309 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62383\u001b[0m\u001b[0m | time: 32.531s\n",
      "| Momentum | epoch: 002 | loss: 0.62383 - acc: 0.9000 | val_loss: 0.69295 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68050\u001b[0m\u001b[0m | time: 32.526s\n",
      "| Momentum | epoch: 003 | loss: 0.68050 - acc: 0.9818 | val_loss: 0.69275 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.68984\u001b[0m\u001b[0m | time: 30.786s\n",
      "| Momentum | epoch: 004 | loss: 0.68984 - acc: 0.9955 | val_loss: 0.69247 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.69185\u001b[0m\u001b[0m | time: 29.146s\n",
      "| Momentum | epoch: 005 | loss: 0.69185 - acc: 0.9986 | val_loss: 0.69213 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.69225\u001b[0m\u001b[0m | time: 30.892s\n",
      "| Momentum | epoch: 006 | loss: 0.69225 - acc: 0.9995 | val_loss: 0.69174 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.69219\u001b[0m\u001b[0m | time: 31.513s\n",
      "| Momentum | epoch: 007 | loss: 0.69219 - acc: 0.9998 | val_loss: 0.69130 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.69195\u001b[0m\u001b[0m | time: 29.864s\n",
      "| Momentum | epoch: 008 | loss: 0.69195 - acc: 0.9999 | val_loss: 0.69082 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.69161\u001b[0m\u001b[0m | time: 36.645s\n",
      "| Momentum | epoch: 009 | loss: 0.69161 - acc: 1.0000 | val_loss: 0.69029 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.69123\u001b[0m\u001b[0m | time: 30.046s\n",
      "| Momentum | epoch: 010 | loss: 0.69123 - acc: 1.0000 | val_loss: 0.68973 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg16-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/vgg16/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.69080\u001b[0m\u001b[0m | time: 34.553s\n",
      "| Momentum | epoch: 011 | loss: 0.69080 - acc: 1.0000 | val_loss: 0.68923 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.69187\u001b[0m\u001b[0m | time: 32.762s\n",
      "| Momentum | epoch: 012 | loss: 0.69187 - acc: 0.7750 | val_loss: 0.68878 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.69241\u001b[0m\u001b[0m | time: 33.231s\n",
      "| Momentum | epoch: 013 | loss: 0.69241 - acc: 0.6571 | val_loss: 0.68837 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.69272\u001b[0m\u001b[0m | time: 30.460s\n",
      "| Momentum | epoch: 014 | loss: 0.69272 - acc: 0.5929 | val_loss: 0.68800 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.69289\u001b[0m\u001b[0m | time: 30.046s\n",
      "| Momentum | epoch: 015 | loss: 0.69289 - acc: 0.5565 | val_loss: 0.68768 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.69300\u001b[0m\u001b[0m | time: 30.078s\n",
      "| Momentum | epoch: 016 | loss: 0.69300 - acc: 0.5353 | val_loss: 0.68738 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.69306\u001b[0m\u001b[0m | time: 32.319s\n",
      "| Momentum | epoch: 017 | loss: 0.69306 - acc: 0.5226 | val_loss: 0.68712 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.69310\u001b[0m\u001b[0m | time: 31.137s\n",
      "| Momentum | epoch: 018 | loss: 0.69310 - acc: 0.5148 | val_loss: 0.68688 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.69312\u001b[0m\u001b[0m | time: 29.756s\n",
      "| Momentum | epoch: 019 | loss: 0.69312 - acc: 0.5099 | val_loss: 0.68667 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.69312\u001b[0m\u001b[0m | time: 32.594s\n",
      "| Momentum | epoch: 020 | loss: 0.69312 - acc: 0.5067 | val_loss: 0.68648 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_vgg16 = tf.Graph()\n",
    "with g_vgg16.as_default():\n",
    "    vgg16_model = vgg16(IMAGE_DIM, IMAGE_DIM, 1e-4)\n",
    "    train_conv_net(vgg16_model, MODEL_NAME_VGG16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19\n",
    "learning_rate = 10^-4, num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b1.npy', 'preprocessed_TSA_scans-tz1-250-250-b2.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg19-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/vgg19/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 35.195s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.69315 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62383\u001b[0m\u001b[0m | time: 33.688s\n",
      "| Momentum | epoch: 002 | loss: 0.62383 - acc: 0.4500 | val_loss: 0.69314 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68054\u001b[0m\u001b[0m | time: 33.816s\n",
      "| Momentum | epoch: 003 | loss: 0.68054 - acc: 0.4909 | val_loss: 0.69314 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.69000\u001b[0m\u001b[0m | time: 31.562s\n",
      "| Momentum | epoch: 004 | loss: 0.69000 - acc: 0.4977 | val_loss: 0.69314 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.69218\u001b[0m\u001b[0m | time: 31.134s\n",
      "| Momentum | epoch: 005 | loss: 0.69218 - acc: 0.4993 | val_loss: 0.69314 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.69280\u001b[0m\u001b[0m | time: 31.002s\n",
      "| Momentum | epoch: 006 | loss: 0.69280 - acc: 0.4998 | val_loss: 0.69313 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.69301\u001b[0m\u001b[0m | time: 32.405s\n",
      "| Momentum | epoch: 007 | loss: 0.69301 - acc: 0.4999 | val_loss: 0.69313 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.69309\u001b[0m\u001b[0m | time: 31.132s\n",
      "| Momentum | epoch: 008 | loss: 0.69309 - acc: 0.5000 | val_loss: 0.69313 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.69312\u001b[0m\u001b[0m | time: 31.467s\n",
      "| Momentum | epoch: 009 | loss: 0.69312 - acc: 0.5000 | val_loss: 0.69313 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.69313\u001b[0m\u001b[0m | time: 32.892s\n",
      "| Momentum | epoch: 010 | loss: 0.69313 - acc: 0.5000 | val_loss: 0.69313 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg19-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/vgg19/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.69314\u001b[0m\u001b[0m | time: 31.029s\n",
      "| Momentum | epoch: 011 | loss: 0.69314 - acc: 0.5000 | val_loss: 0.69305 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.69313\u001b[0m\u001b[0m | time: 29.789s\n",
      "| Momentum | epoch: 012 | loss: 0.69313 - acc: 0.7250 | val_loss: 0.69290 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.69310\u001b[0m\u001b[0m | time: 31.051s\n",
      "| Momentum | epoch: 013 | loss: 0.69310 - acc: 0.8429 | val_loss: 0.69270 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.69302\u001b[0m\u001b[0m | time: 30.746s\n",
      "| Momentum | epoch: 014 | loss: 0.69302 - acc: 0.9071 | val_loss: 0.69244 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.69289\u001b[0m\u001b[0m | time: 31.437s\n",
      "| Momentum | epoch: 015 | loss: 0.69289 - acc: 0.9435 | val_loss: 0.69214 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.69272\u001b[0m\u001b[0m | time: 30.995s\n",
      "| Momentum | epoch: 016 | loss: 0.69272 - acc: 0.9647 | val_loss: 0.69180 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.69252\u001b[0m\u001b[0m | time: 30.699s\n",
      "| Momentum | epoch: 017 | loss: 0.69252 - acc: 0.9774 | val_loss: 0.69141 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.69227\u001b[0m\u001b[0m | time: 29.521s\n",
      "| Momentum | epoch: 018 | loss: 0.69227 - acc: 0.9852 | val_loss: 0.69098 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.69198\u001b[0m\u001b[0m | time: 31.049s\n",
      "| Momentum | epoch: 019 | loss: 0.69198 - acc: 0.9901 | val_loss: 0.69052 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.69166\u001b[0m\u001b[0m | time: 29.365s\n",
      "| Momentum | epoch: 020 | loss: 0.69166 - acc: 0.9933 | val_loss: 0.69002 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_vgg19 = tf.Graph()\n",
    "with g_vgg19.as_default():\n",
    "    vgg19_model = vgg19(IMAGE_DIM, IMAGE_DIM, 1e-4)\n",
    "    train_conv_net(vgg19_model, MODEL_NAME_VGG19, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "learning_rate = 10^-4, num_epoch = 10, momentum optimizer, softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b2.npy', 'preprocessed_TSA_scans-tz1-250-250-b1.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-logistic-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/logistic/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 1.124s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.79985 - val_acc: 0.4091 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.73609\u001b[0m\u001b[0m | time: 1.025s\n",
      "| Momentum | epoch: 002 | loss: 0.73609 - acc: 0.2045 | val_loss: 0.79361 - val_acc: 0.4091 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.79922\u001b[0m\u001b[0m | time: 1.034s\n",
      "| Momentum | epoch: 003 | loss: 0.79922 - acc: 0.2231 | val_loss: 0.78482 - val_acc: 0.4545 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.80922\u001b[0m\u001b[0m | time: 1.031s\n",
      "| Momentum | epoch: 004 | loss: 0.80922 - acc: 0.1921 | val_loss: 0.77376 - val_acc: 0.5000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.80363\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 005 | loss: 0.80363 - acc: 0.2165 | val_loss: 0.76082 - val_acc: 0.5000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.78715\u001b[0m\u001b[0m | time: 1.032s\n",
      "| Momentum | epoch: 006 | loss: 0.78715 - acc: 0.2526 | val_loss: 0.74620 - val_acc: 0.5000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.78389\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 007 | loss: 0.78389 - acc: 0.2374 | val_loss: 0.73017 - val_acc: 0.5455 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.77454\u001b[0m\u001b[0m | time: 1.026s\n",
      "| Momentum | epoch: 008 | loss: 0.77454 - acc: 0.2317 | val_loss: 0.71297 - val_acc: 0.5455 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.76511\u001b[0m\u001b[0m | time: 1.026s\n",
      "| Momentum | epoch: 009 | loss: 0.76511 - acc: 0.2294 | val_loss: 0.69495 - val_acc: 0.5909 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.75424\u001b[0m\u001b[0m | time: 1.028s\n",
      "| Momentum | epoch: 010 | loss: 0.75424 - acc: 0.2510 | val_loss: 0.67626 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-logistic-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/logistic/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.74171\u001b[0m\u001b[0m | time: 1.080s\n",
      "| Momentum | epoch: 011 | loss: 0.74171 - acc: 0.3044 | val_loss: 0.65906 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.71210\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 012 | loss: 0.71210 - acc: 0.3924 | val_loss: 0.64370 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.71979\u001b[0m\u001b[0m | time: 1.025s\n",
      "| Momentum | epoch: 013 | loss: 0.71979 - acc: 0.4580 | val_loss: 0.62999 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.72474\u001b[0m\u001b[0m | time: 1.027s\n",
      "| Momentum | epoch: 014 | loss: 0.72474 - acc: 0.4566 | val_loss: 0.61777 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.72569\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 015 | loss: 0.72569 - acc: 0.4558 | val_loss: 0.60696 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.72761\u001b[0m\u001b[0m | time: 1.027s\n",
      "| Momentum | epoch: 016 | loss: 0.72761 - acc: 0.4553 | val_loss: 0.59736 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.72583\u001b[0m\u001b[0m | time: 1.028s\n",
      "| Momentum | epoch: 017 | loss: 0.72583 - acc: 0.4387 | val_loss: 0.58896 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.72710\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 018 | loss: 0.72710 - acc: 0.4284 | val_loss: 0.58152 - val_acc: 0.7273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.72362\u001b[0m\u001b[0m | time: 1.028s\n",
      "| Momentum | epoch: 019 | loss: 0.72362 - acc: 0.4220 | val_loss: 0.57495 - val_acc: 0.7727 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.72081\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 020 | loss: 0.72081 - acc: 0.4178 | val_loss: 0.56917 - val_acc: 0.7727 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_logistic = tf.Graph()\n",
    "with g_logistic.as_default():\n",
    "    logistic_regression_model = logistic_regression_net(IMAGE_DIM, IMAGE_DIM, 1e-4, 'softmax', 'momentum')\n",
    "    train_conv_net(logistic_regression_model, MODEL_NAME_LOGISTIC, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet - increasing learning rate\n",
    "learning_rate = 10^-3, num_epoch = 10 \n",
    "We can see that this learning rate is too high, as the loss on the validation set is decreasing, and then increasing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b2.npy', 'preprocessed_TSA_scans-tz1-250-250-b1.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg16-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/alexnet/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 12.909s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.60426 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.57977\u001b[0m\u001b[0m | time: 14.018s\n",
      "| Momentum | epoch: 002 | loss: 0.57977 - acc: 0.5318 | val_loss: 0.53814 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.59941\u001b[0m\u001b[0m | time: 12.454s\n",
      "| Momentum | epoch: 003 | loss: 0.59941 - acc: 0.8405 | val_loss: 0.45386 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.56227\u001b[0m\u001b[0m | time: 12.047s\n",
      "| Momentum | epoch: 004 | loss: 0.56227 - acc: 0.8579 | val_loss: 0.36258 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.50098\u001b[0m\u001b[0m | time: 12.113s\n",
      "| Momentum | epoch: 005 | loss: 0.50098 - acc: 0.9563 | val_loss: 0.27443 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.42741\u001b[0m\u001b[0m | time: 12.175s\n",
      "| Momentum | epoch: 006 | loss: 0.42741 - acc: 0.9844 | val_loss: 0.19804 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.35298\u001b[0m\u001b[0m | time: 12.539s\n",
      "| Momentum | epoch: 007 | loss: 0.35298 - acc: 0.9938 | val_loss: 0.13846 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.27152\u001b[0m\u001b[0m | time: 12.617s\n",
      "| Momentum | epoch: 008 | loss: 0.27152 - acc: 0.9973 | val_loss: 0.09480 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.21415\u001b[0m\u001b[0m | time: 13.586s\n",
      "| Momentum | epoch: 009 | loss: 0.21415 - acc: 0.9987 | val_loss: 0.06416 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.17138\u001b[0m\u001b[0m | time: 12.684s\n",
      "| Momentum | epoch: 010 | loss: 0.17138 - acc: 0.9994 | val_loss: 0.04351 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg16-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/alexnet/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.12949\u001b[0m\u001b[0m | time: 12.658s\n",
      "| Momentum | epoch: 011 | loss: 0.12949 - acc: 0.9997 | val_loss: 0.03341 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.77914\u001b[0m\u001b[0m | time: 12.217s\n",
      "| Momentum | epoch: 012 | loss: 0.77914 - acc: 0.7748 | val_loss: 0.02892 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.17206\u001b[0m\u001b[0m | time: 12.132s\n",
      "| Momentum | epoch: 013 | loss: 1.17206 - acc: 0.6570 | val_loss: 0.02796 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.44211\u001b[0m\u001b[0m | time: 11.958s\n",
      "| Momentum | epoch: 014 | loss: 1.44211 - acc: 0.5928 | val_loss: 0.02983 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.58024\u001b[0m\u001b[0m | time: 12.027s\n",
      "| Momentum | epoch: 015 | loss: 1.58024 - acc: 0.5565 | val_loss: 0.03462 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.65413\u001b[0m\u001b[0m | time: 11.887s\n",
      "| Momentum | epoch: 016 | loss: 1.65413 - acc: 0.5353 | val_loss: 0.04305 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.64857\u001b[0m\u001b[0m | time: 11.814s\n",
      "| Momentum | epoch: 017 | loss: 1.64857 - acc: 0.5226 | val_loss: 0.05648 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.57846\u001b[0m\u001b[0m | time: 12.353s\n",
      "| Momentum | epoch: 018 | loss: 1.57846 - acc: 0.5148 | val_loss: 0.07679 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.51891\u001b[0m\u001b[0m | time: 12.242s\n",
      "| Momentum | epoch: 019 | loss: 1.51891 - acc: 0.5098 | val_loss: 0.10646 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.48248\u001b[0m\u001b[0m | time: 13.079s\n",
      "| Momentum | epoch: 020 | loss: 1.48248 - acc: 0.5067 | val_loss: 0.14776 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_alexnet_lr_large = tf.Graph()\n",
    "with g_alexnet_lr_large.as_default():\n",
    "    alexnet_model = alexnet(IMAGE_DIM, IMAGE_DIM, 1e-3)\n",
    "    train_conv_net(alexnet_model, MODEL_NAME_VGG16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet - decreasing learning rate\n",
    "learning_rate = 10^-5, num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b1.npy', 'preprocessed_TSA_scans-tz1-250-250-b2.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg16-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/alexnet/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 13.708s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.57813 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62675\u001b[0m\u001b[0m | time: 12.805s\n",
      "| Momentum | epoch: 002 | loss: 0.62675 - acc: 0.4909 | val_loss: 0.57818 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68658\u001b[0m\u001b[0m | time: 12.504s\n",
      "| Momentum | epoch: 003 | loss: 0.68658 - acc: 0.4983 | val_loss: 0.57836 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.72568\u001b[0m\u001b[0m | time: 12.107s\n",
      "| Momentum | epoch: 004 | loss: 0.72568 - acc: 0.4996 | val_loss: 0.57857 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.74200\u001b[0m\u001b[0m | time: 13.344s\n",
      "| Momentum | epoch: 005 | loss: 0.74200 - acc: 0.4369 | val_loss: 0.57880 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.72693\u001b[0m\u001b[0m | time: 12.743s\n",
      "| Momentum | epoch: 006 | loss: 0.72693 - acc: 0.5651 | val_loss: 0.57906 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.69403\u001b[0m\u001b[0m | time: 13.622s\n",
      "| Momentum | epoch: 007 | loss: 0.69403 - acc: 0.5533 | val_loss: 0.57935 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.72081\u001b[0m\u001b[0m | time: 12.512s\n",
      "| Momentum | epoch: 008 | loss: 0.72081 - acc: 0.5489 | val_loss: 0.57965 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.71507\u001b[0m\u001b[0m | time: 12.529s\n",
      "| Momentum | epoch: 009 | loss: 0.71507 - acc: 0.5471 | val_loss: 0.57997 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.69286\u001b[0m\u001b[0m | time: 12.368s\n",
      "| Momentum | epoch: 010 | loss: 0.69286 - acc: 0.5917 | val_loss: 0.58027 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-vgg16-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/alexnet/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.68393\u001b[0m\u001b[0m | time: 12.691s\n",
      "| Momentum | epoch: 011 | loss: 0.68393 - acc: 0.5913 | val_loss: 0.58019 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.64314\u001b[0m\u001b[0m | time: 12.718s\n",
      "| Momentum | epoch: 012 | loss: 0.64314 - acc: 0.6730 | val_loss: 0.57977 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.64577\u001b[0m\u001b[0m | time: 12.622s\n",
      "| Momentum | epoch: 013 | loss: 0.64577 - acc: 0.6962 | val_loss: 0.57902 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.61990\u001b[0m\u001b[0m | time: 12.628s\n",
      "| Momentum | epoch: 014 | loss: 0.61990 - acc: 0.7461 | val_loss: 0.57800 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.60016\u001b[0m\u001b[0m | time: 12.471s\n",
      "| Momentum | epoch: 015 | loss: 0.60016 - acc: 0.7921 | val_loss: 0.57673 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.59017\u001b[0m\u001b[0m | time: 12.165s\n",
      "| Momentum | epoch: 016 | loss: 0.59017 - acc: 0.8189 | val_loss: 0.57524 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.58543\u001b[0m\u001b[0m | time: 12.403s\n",
      "| Momentum | epoch: 017 | loss: 0.58543 - acc: 0.8350 | val_loss: 0.57354 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.58333\u001b[0m\u001b[0m | time: 12.117s\n",
      "| Momentum | epoch: 018 | loss: 0.58333 - acc: 0.8135 | val_loss: 0.57166 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.58386\u001b[0m\u001b[0m | time: 12.415s\n",
      "| Momentum | epoch: 019 | loss: 0.58386 - acc: 0.8150 | val_loss: 0.56963 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.58740\u001b[0m\u001b[0m | time: 12.186s\n",
      "| Momentum | epoch: 020 | loss: 0.58740 - acc: 0.7868 | val_loss: 0.56746 - val_acc: 1.0000 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_alexnet_lr_small = tf.Graph()\n",
    "with g_alexnet_lr_small.as_default():\n",
    "    alexnet_model = alexnet(IMAGE_DIM, IMAGE_DIM, 1e-5)\n",
    "    train_conv_net(alexnet_model, MODEL_NAME_VGG16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - increasing the number of epochs\n",
    "learning_rate = 10^-4, num_epoch = 50, momentum optimizer, softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b2.npy', 'preprocessed_TSA_scans-tz1-250-250-b1.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-logistic-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/logistic/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 1.111s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.64498 - val_acc: 0.6818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.53971\u001b[0m\u001b[0m | time: 1.031s\n",
      "| Momentum | epoch: 002 | loss: 0.53971 - acc: 0.6545 | val_loss: 0.64072 - val_acc: 0.7273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.60036\u001b[0m\u001b[0m | time: 1.026s\n",
      "| Momentum | epoch: 003 | loss: 0.60036 - acc: 0.6769 | val_loss: 0.63470 - val_acc: 0.7273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.63564\u001b[0m\u001b[0m | time: 1.033s\n",
      "| Momentum | epoch: 004 | loss: 0.63564 - acc: 0.6124 | val_loss: 0.62713 - val_acc: 0.7727 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.61513\u001b[0m\u001b[0m | time: 1.034s\n",
      "| Momentum | epoch: 005 | loss: 0.61513 - acc: 0.6605 | val_loss: 0.61829 - val_acc: 0.7727 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.60392\u001b[0m\u001b[0m | time: 1.025s\n",
      "| Momentum | epoch: 006 | loss: 0.60392 - acc: 0.6450 | val_loss: 0.60834 - val_acc: 0.7727 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.59765\u001b[0m\u001b[0m | time: 1.034s\n",
      "| Momentum | epoch: 007 | loss: 0.59765 - acc: 0.6944 | val_loss: 0.59760 - val_acc: 0.8182 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.58184\u001b[0m\u001b[0m | time: 1.029s\n",
      "| Momentum | epoch: 008 | loss: 0.58184 - acc: 0.7640 | val_loss: 0.58611 - val_acc: 0.8182 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.57956\u001b[0m\u001b[0m | time: 1.034s\n",
      "| Momentum | epoch: 009 | loss: 0.57956 - acc: 0.7686 | val_loss: 0.57393 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.57948\u001b[0m\u001b[0m | time: 1.029s\n",
      "| Momentum | epoch: 010 | loss: 0.57948 - acc: 0.7707 | val_loss: 0.56134 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-logistic-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/logistic/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.56987\u001b[0m\u001b[0m | time: 1.109s\n",
      "| Momentum | epoch: 011 | loss: 0.56987 - acc: 0.7932 | val_loss: 0.55086 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.65317\u001b[0m\u001b[0m | time: 1.032s\n",
      "| Momentum | epoch: 012 | loss: 0.65317 - acc: 0.6817 | val_loss: 0.54214 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.70302\u001b[0m\u001b[0m | time: 1.033s\n",
      "| Momentum | epoch: 013 | loss: 0.70302 - acc: 0.6233 | val_loss: 0.53474 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.72402\u001b[0m\u001b[0m | time: 1.028s\n",
      "| Momentum | epoch: 014 | loss: 0.72402 - acc: 0.6101 | val_loss: 0.52873 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.74207\u001b[0m\u001b[0m | time: 1.034s\n",
      "| Momentum | epoch: 015 | loss: 0.74207 - acc: 0.5848 | val_loss: 0.52395 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.74921\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 016 | loss: 0.74921 - acc: 0.5700 | val_loss: 0.52027 - val_acc: 0.8636 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.75534\u001b[0m\u001b[0m | time: 1.029s\n",
      "| Momentum | epoch: 017 | loss: 0.75534 - acc: 0.5612 | val_loss: 0.51758 - val_acc: 0.9091 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.75423\u001b[0m\u001b[0m | time: 1.042s\n",
      "| Momentum | epoch: 018 | loss: 0.75423 - acc: 0.5715 | val_loss: 0.51576 - val_acc: 0.9091 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.75561\u001b[0m\u001b[0m | time: 1.037s\n",
      "| Momentum | epoch: 019 | loss: 0.75561 - acc: 0.5628 | val_loss: 0.51469 - val_acc: 0.9091 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.74824\u001b[0m\u001b[0m | time: 1.027s\n",
      "| Momentum | epoch: 020 | loss: 0.74824 - acc: 0.5718 | val_loss: 0.51431 - val_acc: 0.9091 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_logistic_epoch50 = tf.Graph()\n",
    "with g_logistic_epoch50.as_default():\n",
    "    logistic_regression_model = logistic_regression_net(IMAGE_DIM, IMAGE_DIM, 1e-4, 'softmax', 'momentum')\n",
    "    train_conv_net(logistic_regression_model, MODEL_NAME_LOGISTIC, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - changing activation function to tanh\n",
    "learning_rate = 10^-4, num_epoch = 10, momentum optimizer, tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed_TSA_scans-tz1-250-250-b1.npy', 'preprocessed_TSA_scans-tz1-250-250-b2.npy']\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-logistic-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/logistic/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 1  | time: 1.094s\n",
      "| Momentum | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 4.59885 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m4.12301\u001b[0m\u001b[0m | time: 1.032s\n",
      "| Momentum | epoch: 002 | loss: 4.12301 - acc: 0.6136 | val_loss: 4.60037 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m4.39106\u001b[0m\u001b[0m | time: 1.033s\n",
      "| Momentum | epoch: 003 | loss: 4.39106 - acc: 0.6322 | val_loss: 4.60174 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m5.24772\u001b[0m\u001b[0m | time: 1.031s\n",
      "| Momentum | epoch: 004 | loss: 5.24772 - acc: 0.6012 | val_loss: 4.60423 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m4.67110\u001b[0m\u001b[0m | time: 1.033s\n",
      "| Momentum | epoch: 005 | loss: 4.67110 - acc: 0.6256 | val_loss: 4.60925 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m5.79963\u001b[0m\u001b[0m | time: 1.031s\n",
      "| Momentum | epoch: 006 | loss: 5.79963 - acc: 0.6325 | val_loss: 4.61609 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m4.95391\u001b[0m\u001b[0m | time: 1.033s\n",
      "| Momentum | epoch: 007 | loss: 4.95391 - acc: 0.6348 | val_loss: 4.62835 - val_acc: 0.2273 -- iter: 22/22\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m4.63058\u001b[0m\u001b[0m | time: 1.030s\n",
      "| Momentum | epoch: 008 | loss: 4.63058 - acc: 0.6357 | val_loss: 2.74573 - val_acc: 0.1818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m4.49144\u001b[0m\u001b[0m | time: 1.028s\n",
      "| Momentum | epoch: 009 | loss: 4.49144 - acc: 0.6360 | val_loss: 2.71782 - val_acc: 0.1818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m4.42448\u001b[0m\u001b[0m | time: 1.031s\n",
      "| Momentum | epoch: 010 | loss: 4.42448 - acc: 0.6135 | val_loss: 2.71428 - val_acc: 0.1818 -- iter: 22/22\n",
      "--\n",
      "None\n",
      "Feature Batch Shape -> (22, 250, 250, 1)\n",
      "training model: \n",
      "---------------------------------\n",
      "Run id: tsa-logistic-v0.1-lr-0.0001-250-250-tz-1\n",
      "Log directory: tsa_logs/train/logistic/\n",
      "---------------------------------\n",
      "Training samples: 22\n",
      "Validation samples: 22\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m4.38799\u001b[0m\u001b[0m | time: 1.082s\n",
      "| Momentum | epoch: 011 | loss: 4.38799 - acc: 0.6028 | val_loss: 2.73017 - val_acc: 0.1818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m4.55575\u001b[0m\u001b[0m | time: 1.036s\n",
      "| Momentum | epoch: 012 | loss: 4.55575 - acc: 0.5361 | val_loss: 3.67989 - val_acc: 0.1818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m4.58473\u001b[0m\u001b[0m | time: 1.031s\n",
      "| Momentum | epoch: 013 | loss: 4.58473 - acc: 0.5011 | val_loss: 3.69694 - val_acc: 0.1818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m4.98335\u001b[0m\u001b[0m | time: 1.026s\n",
      "| Momentum | epoch: 014 | loss: 4.98335 - acc: 0.4821 | val_loss: 4.54587 - val_acc: 0.1818 -- iter: 22/22\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m4.80760\u001b[0m\u001b[0m | time: 1.026s\n",
      "| Momentum | epoch: 015 | loss: 4.80760 - acc: 0.4713 | val_loss: 4.53791 - val_acc: 0.1364 -- iter: 22/22\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m4.32228\u001b[0m\u001b[0m | time: 1.033s\n",
      "| Momentum | epoch: 016 | loss: 4.32228 - acc: 0.4821 | val_loss: 5.59845 - val_acc: 0.1364 -- iter: 22/22\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m4.37781\u001b[0m\u001b[0m | time: 1.029s\n",
      "| Momentum | epoch: 017 | loss: 4.37781 - acc: 0.4394 | val_loss: 6.52968 - val_acc: 0.1364 -- iter: 22/22\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m4.76604\u001b[0m\u001b[0m | time: 1.033s\n",
      "| Momentum | epoch: 018 | loss: 4.76604 - acc: 0.3975 | val_loss: 5.49771 - val_acc: 0.1364 -- iter: 22/22\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m4.70081\u001b[0m\u001b[0m | time: 1.031s\n",
      "| Momentum | epoch: 019 | loss: 4.70081 - acc: 0.3862 | val_loss: 5.45652 - val_acc: 0.0909 -- iter: 22/22\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m4.24743\u001b[0m\u001b[0m | time: 1.026s\n",
      "| Momentum | epoch: 020 | loss: 4.24743 - acc: 0.3205 | val_loss: 5.43032 - val_acc: 0.0909 -- iter: 22/22\n",
      "--\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "g_logistic_tanh = tf.Graph()\n",
    "with g_logistic_tanh.as_default():\n",
    "    logistic_regression_model = logistic_regression_net(IMAGE_DIM, IMAGE_DIM, 1e-4, 'tanh', 'momentum')\n",
    "    train_conv_net(logistic_regression_model, MODEL_NAME_LOGISTIC, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "For this milestone, we looked at some of the state-of-the art neural network architectures for images, comparing the AlexNet with VGG16 and VGG19 architectures, as well as with a basic model of logistic regression.\n",
    "Future work includes further fine-tuning these models, as well as building and training a ResNet and a GoogleNet and making a comparison between all models.\n",
    "Moreover, we were also considering looking into hard and soft-multitask models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
